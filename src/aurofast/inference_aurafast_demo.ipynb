{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404dd271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import os\n",
    "import time\n",
    "from functools import partial\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from torch_geometric.data import (\n",
    "    Batch,\n",
    "    Data,\n",
    "    Dataset,\n",
    ")\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from tqdm import tqdm\n",
    "from transformers import EsmModel, EsmTokenizer\n",
    "try:\n",
    "    import rdkit.Chem as Chem\n",
    "    import rdkit.Chem.AllChem as AllChem\n",
    "except ImportError:\n",
    "    print(\"Warning: RDKit not found. Please install it (`pip install rdkit-pypi`)\")\n",
    "    exit()\n",
    "\n",
    "import numpy as np\n",
    "from rdkit import rdBase\n",
    "\n",
    "rdBase.DisableLog(\"rdApp.warning\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" \n",
    "os.makedirs(\"esm_save\", exist_ok=True)\n",
    "os.makedirs(\"process\", exist_ok=True)\n",
    "import torch\n",
    "# 示例\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        total = torch.cuda.get_device_properties(i).total_memory / 1024**3  # GB\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        print(f\"GPU {i}: Total: {total:.2f} GB | Reserved: {reserved:.2f} GB | Allocated: {allocated:.2f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71da8220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_esm_model(model_name, device):\n",
    "    \"\"\"Loads the ESM tokenizer and model.\"\"\"\n",
    "    print(f\"Loading ESM tokenizer for {model_name}...\")\n",
    "    tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "    print(f\"Loading ESM model {model_name}...\")\n",
    "    model = EsmModel.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "    print(f\"ESM Model loaded on {device}.\")\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "# --- Function to Get ESM Embeddings ---\n",
    "def get_esm_embedding(sequence: str, model, tokenizer, device):\n",
    "    \"\"\"Generates per-residue ESM embeddings for a given sequence.\"\"\"\n",
    "    if not isinstance(sequence, str) or not sequence:\n",
    "        # print(\"Warning: Skipping empty or non-string sequence for ESM.\")\n",
    "        return None\n",
    "    try:\n",
    "        inputs = tokenizer(sequence, return_tensors=\"pt\", add_special_tokens=True)\n",
    "        inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "        last_hidden_states = outputs.hidden_states[-1]\n",
    "        embeddings_all_tokens = last_hidden_states.squeeze(0)\n",
    "        embeddings_residues_only = embeddings_all_tokens[1:-1, :]\n",
    "        if embeddings_residues_only.shape[0] != len(sequence):\n",
    "            print(\n",
    "                f\"Warning: ESM embedding length mismatch for sequence {sequence[:30]}...\"\n",
    "            )\n",
    "        return embeddings_residues_only.cpu()  # Return on CPU\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating ESM embedding for sequence {sequence[:50]}...: {e}\")\n",
    "        # traceback.print_exc() # Uncomment for full traceback\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Function to Compute and Save ESM Embeddings ---\n",
    "def compute_and_save_esm(csv_path, seq_col, pkl_path, model, tokenizer, device):\n",
    "    \"\"\"Computes ESM embeddings for unique sequences in CSV and saves to HDF5.\"\"\"\n",
    "    print(\"\\n--- Starting ESM Embedding Computation ---\")\n",
    "    print(f\"Loading CSV file: {csv_path}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV {csv_path}: {e}\")\n",
    "        return\n",
    "    if seq_col not in df.columns:\n",
    "        print(f\"Error: Sequence column '{seq_col}' not found in CSV.\")\n",
    "        return\n",
    "\n",
    "    unique_sequences = df[seq_col].dropna().unique()\n",
    "    print(f\"Found {len(unique_sequences)} unique protein sequences.\")\n",
    "    print(f\"Saving ESM embeddings to file: {pkl_path}...\")\n",
    "\n",
    "    esm_embedding_dict = {}\n",
    "    for seq in tqdm(unique_sequences, desc=\"Computing ESM embeddings\"):\n",
    "        if seq in esm_embedding_dict.keys():\n",
    "            pass\n",
    "        else:\n",
    "            embedding = get_esm_embedding(seq, model, tokenizer, device)\n",
    "            if embedding is not None:\n",
    "                esm_embedding_dict[seq] = embedding\n",
    "            else:\n",
    "                print(f\"Failed to compute embedding for sequence: {seq[:50]}...\")\n",
    "\n",
    "    print(\"ESM Embeddings computation and saving finished.\")\n",
    "    torch.save(esm_embedding_dict, pkl_path)\n",
    "\n",
    "\n",
    "# --- NEW: Function to Precompute Fingerprints ---\n",
    "def precompute_fingerprints(csv_path, smiles_col, radius, n_bits, output_pkl_path):\n",
    "    \"\"\"\n",
    "    Computes Morgan fingerprints for unique SMILES in a CSV and saves them to a pickle file.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the input CSV file.\n",
    "        smiles_col (str): Name of the column containing SMILES strings.\n",
    "        radius (int): Morgan fingerprint radius.\n",
    "        n_bits (int): Morgan fingerprint size (number of bits).\n",
    "        output_pkl_path (str): Path to save the output dictionary (.pkl file).\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Fingerprint Precomputation ---\")\n",
    "    print(f\"Loading CSV file: {csv_path}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV {csv_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    if smiles_col not in df.columns:\n",
    "        print(f\"Error: SMILES column '{smiles_col}' not found in CSV.\")\n",
    "        return\n",
    "\n",
    "    # Get unique, non-empty SMILES strings\n",
    "    unique_smiles = df[smiles_col].dropna().unique()\n",
    "    print(f\"Found {len(unique_smiles)} unique SMILES strings.\")\n",
    "    print(f\"Calculating MorganFP(radius={radius}, nBits={n_bits})\")\n",
    "\n",
    "    fingerprint_dict = {}\n",
    "    computed_count = 0\n",
    "    failed_count = 0\n",
    "\n",
    "    for smiles in tqdm(unique_smiles, desc=\"Computing fingerprints\"):\n",
    "        if not isinstance(smiles, str) or not smiles:\n",
    "            failed_count += 1\n",
    "            continue\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                # print(f\"Warning: RDKit failed to parse SMILES: {smiles[:50]}...\")\n",
    "                failed_count += 1\n",
    "                continue\n",
    "\n",
    "            # Calculate fingerprint\n",
    "            fp_bitvect = AllChem.GetMorganFingerprintAsBitVect(\n",
    "                mol, radius=radius, nBits=n_bits\n",
    "            )\n",
    "            # Convert to NumPy array (float32 is common for ML)\n",
    "            fp_array = np.array(fp_bitvect, dtype=np.float32)\n",
    "\n",
    "            # Store in dictionary (SMILES -> fingerprint array)\n",
    "            fingerprint_dict[smiles] = fp_array\n",
    "            computed_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing fingerprint for SMILES {smiles[:50]}...: {e}\")\n",
    "            # traceback.print_exc() # Uncomment for full traceback\n",
    "            failed_count += 1\n",
    "\n",
    "    print(\"Fingerprint computation finished.\")\n",
    "    print(f\"Successfully computed: {computed_count}, Failed: {failed_count}\")\n",
    "\n",
    "    # Save the dictionary to a pickle file\n",
    "    print(f\"Saving fingerprint dictionary to: {output_pkl_path}...\")\n",
    "    try:\n",
    "        torch.save(fingerprint_dict, output_pkl_path)\n",
    "        print(\"Fingerprint dictionary saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving fingerprint dictionary: {e}\")\n",
    "\n",
    "\n",
    "def precompute_atom_counts(csv_path, smiles_col, output_pkl_path):\n",
    "    \"\"\"\n",
    "    Computes the number of atoms for each unique SMILES string in a CSV\n",
    "    and saves the results (SMILES -> atom_count) to a pickle file.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the input CSV file.\n",
    "        smiles_col (str): Name of the column containing SMILES strings.\n",
    "        output_pkl_path (str): Path to save the output dictionary (.pkl file).\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Atom Count Precomputation ---\")\n",
    "    print(f\"Loading CSV file: {csv_path}...\")\n",
    "    try:\n",
    "        # Load only the necessary SMILES column to save memory if the CSV is large\n",
    "        df = pd.read_csv(csv_path, usecols=[smiles_col])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input CSV file not found at {csv_path}\")\n",
    "        return\n",
    "    except ValueError as e:\n",
    "        # Handle case where smiles_col is not in the CSV\n",
    "        print(f\"Error reading CSV: {e}. Ensure '{smiles_col}' column exists.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV {csv_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Get unique, non-empty SMILES strings\n",
    "    try:\n",
    "        unique_smiles = df[smiles_col].dropna().unique()\n",
    "        print(f\"Found {len(unique_smiles)} unique SMILES strings.\")\n",
    "    except KeyError:\n",
    "        print(\n",
    "            f\"Error: SMILES column '{smiles_col}' not found after loading. Check column name again.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    atom_count_dict = {}\n",
    "    computed_count = 0\n",
    "    failed_count = 0\n",
    "\n",
    "    for smiles in tqdm(unique_smiles, desc=\"Computing atom counts\"):\n",
    "        if not isinstance(smiles, str) or not smiles:\n",
    "            # Skip non-string or empty entries silently or add a counter\n",
    "            failed_count += 1\n",
    "            continue\n",
    "        try:\n",
    "            # Create molecule object from SMILES\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "            if mol is None:\n",
    "                # print(f\"Warning: RDKit failed to parse SMILES: {smiles[:50]}...\")\n",
    "                failed_count += 1\n",
    "                continue\n",
    "\n",
    "            # Get the number of atoms\n",
    "            num_atoms = mol.GetNumAtoms()\n",
    "\n",
    "            # Store in dictionary (SMILES -> atom_count)\n",
    "            atom_count_dict[smiles] = num_atoms\n",
    "            computed_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            # Catch potential errors during RDKit processing for a specific SMILES\n",
    "            print(f\"Error computing atom count for SMILES {smiles[:50]}...: {e}\")\n",
    "            # traceback.print_exc() # Uncomment for full traceback if needed\n",
    "            failed_count += 1\n",
    "\n",
    "    print(\"Atom count computation finished.\")\n",
    "    print(\n",
    "        f\"Successfully computed: {computed_count}, Failed to parse/process: {failed_count}\"\n",
    "    )\n",
    "\n",
    "    # Save the dictionary to a pickle file\n",
    "    print(f\"Saving atom count dictionary to: {output_pkl_path}...\")\n",
    "    try:\n",
    "        # Ensure the output directory exists\n",
    "        torch.save(atom_count_dict, output_pkl_path)\n",
    "        print(\"Atom count dictionary saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving atom count dictionary: {e}\")\n",
    "\n",
    "\n",
    "def precompute_atom_types(csv_path, smiles_col, output_pkl_path):\n",
    "    \"\"\"\n",
    "    Computes the list of atom types (atomic numbers) for each unique SMILES string\n",
    "    in a CSV and saves the results (SMILES -> list_of_atom_types) to a pickle file.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the input CSV file.\n",
    "        smiles_col (str): Name of the column containing SMILES strings.\n",
    "        output_pkl_path (str): Path to save the output dictionary (.pkl file).\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Atom Types Precomputation ---\")\n",
    "    print(f\"Loading CSV file: {csv_path}...\")\n",
    "    try:\n",
    "        # Load only the necessary SMILES column to save memory if the CSV is large\n",
    "        df = pd.read_csv(csv_path, usecols=[smiles_col])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input CSV file not found at {csv_path}\")\n",
    "        return\n",
    "    except ValueError as e:\n",
    "        # Handle case where smiles_col is not in the CSV\n",
    "        print(f\"Error reading CSV: {e}. Ensure '{smiles_col}' column exists.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV {csv_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Get unique, non-empty SMILES strings\n",
    "    try:\n",
    "        unique_smiles = df[smiles_col].dropna().unique()\n",
    "        print(f\"Found {len(unique_smiles)} unique SMILES strings.\")\n",
    "    except KeyError:\n",
    "        print(\n",
    "            f\"Error: SMILES column '{smiles_col}' not found after loading. Check column name again.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    atom_types_dict = {}  # Renamed dictionary\n",
    "    computed_count = 0\n",
    "    failed_count = 0\n",
    "\n",
    "    for smiles in tqdm(unique_smiles, desc=\"Computing atom types\"):\n",
    "        if not isinstance(smiles, str) or not smiles:\n",
    "            # Skip non-string or empty entries silently or add a counter\n",
    "            failed_count += 1\n",
    "            continue\n",
    "        try:\n",
    "            # Create molecule object from SMILES\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "            if mol is None:\n",
    "                # print(f\"Warning: RDKit failed to parse SMILES: {smiles[:50]}...\")\n",
    "                failed_count += 1\n",
    "                continue\n",
    "\n",
    "            # --- MODIFIED PART: Get list of atomic numbers ---\n",
    "            atom_types = [atom.GetAtomicNum() for atom in mol.GetAtoms()]\n",
    "            # --- End of MODIFIED PART ---\n",
    "\n",
    "            # Store in dictionary (SMILES -> list of atom types)\n",
    "            atom_types_dict[smiles] = atom_types\n",
    "            computed_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            # Catch potential errors during RDKit processing for a specific SMILES\n",
    "            print(f\"Error computing atom types for SMILES {smiles[:50]}...: {e}\")\n",
    "            # traceback.print_exc() # Uncomment for full traceback if needed\n",
    "            failed_count += 1\n",
    "\n",
    "    print(\"Atom type computation finished.\")\n",
    "    print(\n",
    "        f\"Successfully computed: {computed_count}, Failed to parse/process: {failed_count}\"\n",
    "    )\n",
    "\n",
    "    # Save the dictionary to a pickle file\n",
    "    print(f\"Saving atom types dictionary to: {output_pkl_path}...\")\n",
    "    try:\n",
    "        # Ensure the output directory exists\n",
    "        torch.save(atom_types_dict, output_pkl_path)\n",
    "        print(\"Atom types dictionary saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving atom types dictionary: {e}\")\n",
    "\n",
    "\n",
    "# --- Worker Function (must be defined at top-level or be picklable) ---\n",
    "def _compute_single_fingerprint(smiles, radius, n_bits):\n",
    "    \"\"\"\n",
    "    Computes the Morgan fingerprint for a single SMILES string.\n",
    "\n",
    "    Args:\n",
    "        smiles (str): The SMILES string.\n",
    "        radius (int): Morgan fingerprint radius.\n",
    "        n_bits (int): Morgan fingerprint size.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (smiles, np.array or None) - The original SMILES and its\n",
    "               computed fingerprint as a float32 numpy array, or None if failed.\n",
    "    \"\"\"\n",
    "    if not isinstance(smiles, str) or not smiles:\n",
    "        return smiles, None  # Return smiles to track failures\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            # Optionally log or handle SMILES parsing errors more explicitly here\n",
    "            # print(f\"Warning: RDKit failed to parse SMILES: {smiles[:50]}...\")\n",
    "            return smiles, None\n",
    "\n",
    "        # Calculate fingerprint\n",
    "        fp_bitvect = AllChem.GetMorganFingerprintAsBitVect(\n",
    "            mol, radius=radius, nBits=n_bits\n",
    "        )\n",
    "        # Convert to NumPy array (float32 is common for ML)\n",
    "        fp_array = np.array(fp_bitvect, dtype=np.float32)\n",
    "        return smiles, fp_array\n",
    "\n",
    "    except Exception:\n",
    "        # Catch errors during fingerprint calculation for a specific SMILES\n",
    "        # print(f\"Error computing fingerprint for SMILES {smiles[:50]}...: {e}\")\n",
    "        # traceback.print_exc() # Uncomment for detailed traceback from worker\n",
    "        return smiles, None\n",
    "\n",
    "\n",
    "# --- Main Function using Multiprocessing ---\n",
    "def precompute_fingerprints_mp(\n",
    "    csv_path,\n",
    "    smiles_col,\n",
    "    radius,\n",
    "    n_bits,\n",
    "    output_pkl_path,\n",
    "    num_workers=None,\n",
    "    chunksize=100,\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes Morgan fingerprints in parallel for unique SMILES in a CSV\n",
    "    and saves them to a pickle file using multiprocessing.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the input CSV file.\n",
    "        smiles_col (str): Name of the column containing SMILES strings.\n",
    "        radius (int): Morgan fingerprint radius.\n",
    "        n_bits (int): Morgan fingerprint size (number of bits).\n",
    "        output_pkl_path (str): Path to save the output dictionary (.pkl file).\n",
    "        num_workers (int, optional): Number of worker processes.\n",
    "                                     Defaults to os.cpu_count() - 1.\n",
    "        chunksize (int, optional): Number of tasks to send to each worker at once.\n",
    "                                   Adjusting might impact performance. Defaults to 100.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Fingerprint Precomputation (Multiprocessing) ---\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Loading CSV file: {csv_path}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)  # , usecols=[smiles_col])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input CSV file not found at {csv_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV {csv_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    if smiles_col not in df.columns:\n",
    "        print(f\"Error: SMILES column '{smiles_col}' not found in CSV.\")\n",
    "        return\n",
    "\n",
    "    # Get unique, non-empty SMILES strings\n",
    "    unique_smiles = df[smiles_col].dropna().unique()\n",
    "    total_unique = len(unique_smiles)\n",
    "    print(f\"Found {total_unique} unique SMILES strings.\")\n",
    "\n",
    "    if total_unique == 0:\n",
    "        print(\"No unique SMILES strings found to process.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Calculating MorganFP(radius={radius}, nBits={n_bits})\")\n",
    "\n",
    "    # Determine number of workers\n",
    "    if num_workers is None:\n",
    "        num_workers = os.cpu_count()\n",
    "        if num_workers > 1:\n",
    "            num_workers -= 1  # Leave one core for the main process and OS\n",
    "        print(f\"Defaulting to {num_workers} worker processes.\")\n",
    "    else:\n",
    "        num_workers = max(1, num_workers)\n",
    "        print(f\"Using specified {num_workers} worker processes.\")\n",
    "\n",
    "    worker_func = partial(_compute_single_fingerprint, radius=radius, n_bits=n_bits)\n",
    "\n",
    "    fingerprint_dict = {}\n",
    "    computed_count = 0\n",
    "    failed_count = 0\n",
    "\n",
    "    print(f\"Starting parallel computation with chunksize={chunksize}...\")\n",
    "    try:\n",
    "        with mp.Pool(processes=num_workers) as pool:\n",
    "            results_iterator = pool.imap_unordered(\n",
    "                worker_func, unique_smiles, chunksize=chunksize\n",
    "            )\n",
    "            for smiles_key, fp_result in tqdm(\n",
    "                results_iterator, total=total_unique, desc=\"Computing fingerprints\"\n",
    "            ):\n",
    "                if fp_result is not None:\n",
    "                    fingerprint_dict[smiles_key] = fp_result\n",
    "                    computed_count += 1\n",
    "                else:\n",
    "                    failed_count += 1\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during multiprocessing: {e}\")\n",
    "        return\n",
    "\n",
    "    computation_time = time.time() - start_time\n",
    "    print(f\"\\nFingerprint computation finished in {computation_time:.2f} seconds.\")\n",
    "    print(f\"Successfully computed: {computed_count}, Failed: {failed_count}\")\n",
    "\n",
    "    # Save the dictionary to a pickle file using torch.save\n",
    "    if computed_count > 0:\n",
    "        print(\n",
    "            f\"Saving fingerprint dictionary ({computed_count} entries) to: {output_pkl_path}...\"\n",
    "        )\n",
    "        save_start_time = time.time()\n",
    "        try:\n",
    "            # Using torch.save as in the original function\n",
    "            torch.save(fingerprint_dict, output_pkl_path)\n",
    "            save_time = time.time() - save_start_time\n",
    "            print(\n",
    "                f\"Fingerprint dictionary saved successfully in {save_time:.2f} seconds.\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving fingerprint dictionary: {e}\")\n",
    "    else:\n",
    "        print(\"No fingerprints were successfully computed, skipping save.\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"--- Total precomputation time: {total_time:.2f} seconds ---\")\n",
    "\n",
    "\n",
    "def compute_processed_trunk_file(\n",
    "    original_trunk_file, trunk_file_dir, saved_processed_trunk_file\n",
    "):\n",
    "    df = pd.read_csv(original_trunk_file)\n",
    "    feats_dict = {}\n",
    "    for path in os.listdir(trunk_file_dir):\n",
    "        idx = path.split(\".\")[0].split(\"_\")[-1]\n",
    "        trunk = torch.load(\n",
    "            os.path.join(trunk_file_dir, path), weights_only=False, map_location=\"cpu\"\n",
    "        )\n",
    "        proseq = df[df.ID == int(idx)].PROTEIN_SEQUENCE.item()\n",
    "        feats_dict[proseq] = {\"s\": trunk[\"s\"], \"z\": trunk[\"z\"].mean(dim=1)}\n",
    "    torch.save(feats_dict, saved_processed_trunk_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43c26eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_esm = True\n",
    "compute_fingerprint = True\n",
    "compute_atom_type = True\n",
    "compute_trunk_file = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "esm_csv_file = fingerprint_csv_file = atomtype_csv_file = \"demo_input.csv\"\n",
    "\n",
    "esm_output_file = \"esm_save/screen_esm_embeddings.pkl\"\n",
    "fingerprint_output_file = \"demo_input_fingerprint.pkl\"\n",
    "atomtype_output_file = \"demo_input_atomtype.pkl\"\n",
    "\n",
    "original_trunk_file = \"wait2pred.csv\"\n",
    "trunk_file_dir = \"feats\"\n",
    "saved_processed_trunk_file = \"process/trunk_dict_withseq.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b50d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_esm = True\n",
    "compute_fingerprint = True\n",
    "compute_atom_type = True\n",
    "compute_trunk_file = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device\", device)\n",
    "\n",
    "\n",
    "esm_csv_file = fingerprint_csv_file = atomtype_csv_file = \"demo_input.csv\"\n",
    "\n",
    "esm_output_file = \"esm_save/screen_esm_embeddings.pkl\"\n",
    "fingerprint_output_file = \"demo_input_fingerprint.pkl\"\n",
    "atomtype_output_file = \"demo_input_atomtype.pkl\"\n",
    "\n",
    "original_trunk_file = \"wait2pred.csv\"\n",
    "trunk_file_dir = \"feats\"\n",
    "saved_processed_trunk_file = \"process/trunk_dict_withseq.pkl\"\n",
    "\n",
    "if compute_esm:\n",
    "    # --- Option 1: Compute ESM Embeddings ---\n",
    "    # Uncomment the block below if you need to compute ESM embeddings\n",
    "    print(\"\\nStarting ESM computation...\")\n",
    "    esm_model_name = \"facebook/esm2_t36_3B_UR50D\"\n",
    "    sequence_column = \"Protein_Sequence\"\n",
    "    esm_tokenizer, esm_model = load_esm_model(esm_model_name, device)\n",
    "\n",
    "    compute_and_save_esm(\n",
    "        csv_path=esm_csv_file,\n",
    "        seq_col=sequence_column,\n",
    "        pkl_path=esm_output_file,\n",
    "        model=esm_model,\n",
    "        tokenizer=esm_tokenizer,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    print(\"ESM computation finished.\")\n",
    "\n",
    "\n",
    "if compute_fingerprint:\n",
    "    # --- Option 2: Precompute Fingerprints ---\n",
    "    print(\"\\nStarting Fingerprint precomputation...\")\n",
    "   \n",
    "    smiles_column = \"Canonical_SMILES\"\n",
    "    fp_radius = 2\n",
    "    fp_nbits = 1024\n",
    "    \n",
    "    precompute_fingerprints_mp(\n",
    "        csv_path=fingerprint_csv_file,\n",
    "        smiles_col=smiles_column,\n",
    "        radius=fp_radius,\n",
    "        n_bits=fp_nbits,\n",
    "        output_pkl_path=fingerprint_output_file,\n",
    "        num_workers=400,\n",
    "        chunksize=500,\n",
    "    )\n",
    "\n",
    "    print(\"Fingerprint precomputation finished.\")\n",
    "\n",
    "if compute_atom_type:\n",
    "    # --- Option 2: Precompute Fingerprints ---\n",
    "    print(\"\\nStarting Fingerprint precomputation...\")\n",
    "   \n",
    "    smiles_column = \"Canonical_SMILES\"\n",
    "    \n",
    "    precompute_atom_types(\n",
    "        csv_path=atomtype_csv_file,\n",
    "        smiles_col=smiles_column,\n",
    "        output_pkl_path=atomtype_output_file,\n",
    "    )\n",
    "\n",
    "if compute_processed_trunk_file:\n",
    "    print(\"\\nStarting Trunk File precomputation...\")\n",
    "    compute_processed_trunk_file(\n",
    "        original_trunk_file, trunk_file_dir, saved_processed_trunk_file\n",
    "    )\n",
    "\n",
    "print(\"\\nPrecomputation script finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ded1925",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LigandProteinInteractionModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=1024,\n",
    "        esm_dim=2560,\n",
    "        # Max atomic num + 1\n",
    "        num_atom_types=118,\n",
    "        fp_dim=1024,\n",
    "        graph_output_dim=1,\n",
    "        combiner_dropout=0.2,\n",
    "        predictor_dropout=0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.scaling_factor = 1e-2\n",
    "\n",
    "        # --- Encoders (Same as before) ---\n",
    "        self.atom_encoder = nn.Embedding(num_atom_types, hidden_size)\n",
    "\n",
    "        self.fp_encoder = nn.Sequential(\n",
    "            nn.Linear(fp_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "        )\n",
    "\n",
    "        self.esm_proj = nn.Linear(esm_dim, hidden_size)\n",
    "\n",
    "        # --- Graph Pooling Method ---\n",
    "        self.global_combiner = nn.Sequential(\n",
    "            # Example: Project down slightly\n",
    "            nn.Linear(hidden_size * 3, hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=combiner_dropout),\n",
    "            # Output unified embedding\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "        )\n",
    "\n",
    "        self.trunk_encoder = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "        # --- Final Graph Prediction Head ---\n",
    "        self.graph_predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=predictor_dropout),\n",
    "            nn.Linear(hidden_size // 2, graph_output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, data: Batch):\n",
    "        \"\"\"\n",
    "        Forward pass optimized for graph-level prediction.\n",
    "        Includes atom's own embedding in the feature fusion before context layer.\n",
    "        Uses data.atom_types_batch for ligand atom indexing.\n",
    "        \"\"\"\n",
    "        B = data.num_graphs\n",
    "        # 1. Encode Ligand Atoms (Node features)\n",
    "        atom_emb = self.atom_encoder(data.atom_types)  # [N_total_lig, H]\n",
    "\n",
    "        trunk_s = data.trunk_s\n",
    "        trunk_s = global_mean_pool(\n",
    "            trunk_s.mean(1), batch=data[\"trunk_s_batch\"]\n",
    "        ).unsqueeze(1)\n",
    "        trunk_s = trunk_s.repeat(1, self.hidden_size)\n",
    "        trunk_z = data.trunk_z\n",
    "        trunk_z = global_mean_pool(\n",
    "            trunk_z.mean((1)), batch=data[\"trunk_z_batch\"]\n",
    "        ).unsqueeze(1)\n",
    "        trunk_z = trunk_z.repeat(1, self.hidden_size)\n",
    "        trunk = trunk_s + trunk_z\n",
    "        trunk = self.trunk_encoder(trunk)\n",
    "        trunk = F.sigmoid(trunk) * trunk\n",
    "\n",
    "        # --- Use atom_types_batch ---\n",
    "        # 1.5 Pool initial atoms for global context\n",
    "        ligand_atom_pooled = global_mean_pool(atom_emb, data.atom_types_batch) \n",
    "\n",
    "        # 2. Encode Fingerprint (Graph-level feature)\n",
    "        fp_input = (\n",
    "            data.fp.squeeze(1)\n",
    "            if data.fp.dim() == 3 and data.fp.size(1) == 1\n",
    "            else data.fp\n",
    "        )\n",
    "        fp_emb = self.fp_encoder(fp_input)  # [B, H]\n",
    "\n",
    "        # 3. Encode Protein ESM (Node features pooled to graph level)\n",
    "        # --- Uses esm_embedding_batch (created because it's in follow_batch) ---\n",
    "        if hasattr(data, \"esm_embedding\") and data.esm_embedding.shape[0] > 0: # type: ignore\n",
    "            if not hasattr(data, \"esm_embedding_batch\"):\n",
    "                # This is critical, should be guaranteed by DataLoader's follow_batch\n",
    "                raise AttributeError(\n",
    "                    \"ESM embedding exists but 'esm_embedding_batch' is missing. Check DataLoader setup.\"\n",
    "                )\n",
    "            esm_pooled = global_mean_pool(data.esm_embedding, data.esm_embedding_batch) # type: ignore\n",
    "            esm_ctx = self.esm_proj(esm_pooled)  # [B, H]\n",
    "        else:\n",
    "            # Default to zeros if no protein info\n",
    "            # Ensure device/dtype match other tensors for concatenation\n",
    "            esm_ctx = torch.zeros(\n",
    "                B,\n",
    "                self.hidden_size,\n",
    "                device=fp_emb.device,\n",
    "                dtype=fp_emb.dtype,\n",
    "            )\n",
    "\n",
    "        # esm_ctx = esm_ctx + trunk * self.scaling_factor\n",
    "        esm_ctx = esm_ctx + trunk * self.scaling_factor\n",
    "        # 6. Fuse Features (Dense Format)\n",
    "        combined_global = torch.cat(\n",
    "            [fp_emb, esm_ctx, ligand_atom_pooled], dim=-1\n",
    "        )  # Shape: [B, H * 3]\n",
    "        # Shape: [B, graph_output_dim]\n",
    "        combined_features = self.global_combiner(combined_global)\n",
    "        graph_prediction = self.graph_predictor(combined_features)\n",
    "        return graph_prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3224e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_skip_none(batch: List[Optional[Any]]) -> Batch:\n",
    "    \"\"\"Collate function that filters out None items.\"\"\"\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return Batch() \n",
    "    try:\n",
    "        return Batch.from_data_list(batch)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during collation: {e}. Skipping batch.\")\n",
    "        return Batch()\n",
    "\n",
    "\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        samples_df: pd.DataFrame,\n",
    "        esm_embeddings_dict: dict,\n",
    "        fingerprint_dict: dict,\n",
    "        atom_type_dict: dict,\n",
    "        trunk_dict: dict,\n",
    "        esm_key_col=\"Protein_Sequence\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Dataset for inference using pre-loaded dictionaries.\n",
    "        \"\"\"\n",
    "        super().__init__(root=None)\n",
    "        self.samples_df = samples_df.reset_index(drop=True)\n",
    "        self.esm_embeddings_dict = esm_embeddings_dict\n",
    "        self.fingerprint_dict = fingerprint_dict\n",
    "        self.atom_type_dict = atom_type_dict\n",
    "        self.esm_key_col = esm_key_col\n",
    "        self.trunk_dict = trunk_dict\n",
    "        # Column names\n",
    "        self.smiles_col = \"Canonical_SMILES\"\n",
    "        self.id_col = \"ID\"\n",
    "\n",
    "        # Check for required columns\n",
    "        required_cols = [\n",
    "            self.smiles_col,\n",
    "            self.esm_key_col,\n",
    "            self.id_col,\n",
    "        ]\n",
    "        missing = [c for c in required_cols if c not in self.samples_df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required columns in DataFrame: {missing}\")\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.samples_df)\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\"Gets and processes the idx-th sample using pre-loaded dictionaries.\"\"\"\n",
    "        # This method remains largely the same, processing one sample at a time.\n",
    "        if idx >= len(self.samples_df):\n",
    "            # This should ideally not happen if sampler is used correctly\n",
    "            raise IndexError(\n",
    "                f\"Index {idx} out of bounds for dataset length {len(self.samples_df)}\"\n",
    "            )\n",
    "\n",
    "        row = self.samples_df.iloc[idx]\n",
    "        smiles = row.get(self.smiles_col, None)\n",
    "        protein_key = row.get(self.esm_key_col, None)\n",
    "        sample_id = row.get(self.id_col, f\"sample_{idx}\")\n",
    "        # trunk_idx = int(row.get(\"ID_A\", 0))\n",
    "        # print(trunk_idx)\n",
    "\n",
    "        # 1. Validate SMILES, Protein Key, Target\n",
    "        if not isinstance(smiles, str) or not smiles:\n",
    "            return None\n",
    "        if not isinstance(protein_key, str) or not protein_key:\n",
    "            return None\n",
    "\n",
    "        # 2. Lookup pre-computed Fingerprint\n",
    "        if smiles not in self.fingerprint_dict:\n",
    "            return None\n",
    "        fp_array = self.fingerprint_dict[smiles]\n",
    "        fp_tensor = torch.from_numpy(fp_array).float().unsqueeze(0)\n",
    "\n",
    "        # 3. Lookup pre-computed Atom Types\n",
    "        if smiles not in self.atom_type_dict:\n",
    "            return None\n",
    "        atom_types = self.atom_type_dict[smiles]\n",
    "        atom_types_tensor = torch.tensor(atom_types, dtype=torch.long)\n",
    "        num_lig_atoms = len(atom_types)\n",
    "        if num_lig_atoms == 0:\n",
    "            return None\n",
    "\n",
    "        # 4. Lookup pre-loaded ESM embedding\n",
    "        if protein_key not in self.esm_embeddings_dict:\n",
    "            return None\n",
    "        esm_embedding = self.esm_embeddings_dict[protein_key]\n",
    "        if not isinstance(esm_embedding, torch.Tensor) or esm_embedding.shape[0] == 0:\n",
    "            return None\n",
    "        num_prot_res = esm_embedding.shape[0]\n",
    "\n",
    "        trunk_file = self.trunk_dict[protein_key]\n",
    "\n",
    "        # 6. Create Data object\n",
    "        data = Data(\n",
    "            atom_types=atom_types_tensor,\n",
    "            fp=fp_tensor,\n",
    "            esm_embedding=esm_embedding.clone(),\n",
    "            smiles=smiles,\n",
    "            trunk_s=trunk_file[\"s\"],\n",
    "            trunk_z=trunk_file[\"z\"],\n",
    "            num_lig_atoms=num_lig_atoms,\n",
    "            num_prot_res=num_prot_res,\n",
    "            sample_name=sample_id,\n",
    "        )\n",
    "        return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79eb605",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2048\n",
    "HIDDEN_SIZE = 1024\n",
    "ESM_DIM = 2560\n",
    "FP_DIM = 1024\n",
    "MAX_ATOMIC_NUM = 118\n",
    "GRAPH_OUTPUT_DIM = 1\n",
    "checkpoint_path = \"model_load/model_aurofast.pt\"\n",
    "# device = \"cuda:1\"\n",
    "result_csv_path = \"inference_result.csv\"\n",
    "USE_AMP = True\n",
    "\n",
    "\n",
    "esm_embeddings_ram = torch.load(\n",
    "        esm_output_file,\n",
    "        map_location=\"cpu\",\n",
    "        weights_only=False,\n",
    "    )\n",
    "\n",
    "fingerprint_dict_ram = torch.load(\n",
    "   fingerprint_output_file,\n",
    "    map_location=\"cpu\",\n",
    "    weights_only=False,\n",
    ")\n",
    "\n",
    "atom_type_dict_ram = torch.load(\n",
    "    atomtype_output_file,\n",
    "    map_location=\"cpu\",\n",
    "    weights_only=False,\n",
    ")\n",
    "\n",
    "trunk_dict = torch.load(\n",
    "    saved_processed_trunk_file ,\n",
    "    map_location=\"cpu\",\n",
    "    weights_only=False,\n",
    ")\n",
    "\n",
    "inference_df = pd.read_csv(\"demo_input.csv\")\n",
    "esm_key_col = \"Protein_Sequence\"\n",
    "smiles_col = \"Canonical_SMILES\" \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b988b960",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dataset = InferenceDataset(\n",
    "    samples_df=inference_df,\n",
    "    esm_embeddings_dict=esm_embeddings_ram,\n",
    "    fingerprint_dict=fingerprint_dict_ram,\n",
    "    atom_type_dict=atom_type_dict_ram,\n",
    "    esm_key_col=esm_key_col,\n",
    "    trunk_dict=trunk_dict,\n",
    ")\n",
    "\n",
    "print(f\"Dataset initialized with {len(inference_dataset)} samples.\")\n",
    "\n",
    "loader = DataLoader(\n",
    "    inference_dataset,\n",
    "    batch_size=BATCH_SIZE,  # This is now per-GPU batch size\n",
    "    collate_fn=collate_skip_none,\n",
    "    follow_batch=[\"esm_embedding\", \"atom_types\", \"trunk_s\", \"trunk_z\"],\n",
    ")\n",
    "\n",
    "print(f\"DataLoader setup complete with Batch Size: {BATCH_SIZE}.\")\n",
    "\n",
    "model = LigandProteinInteractionModel(\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    esm_dim=ESM_DIM,\n",
    "    fp_dim=FP_DIM,\n",
    "    num_atom_types=MAX_ATOMIC_NUM + 1,\n",
    "    graph_output_dim=GRAPH_OUTPUT_DIM,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "print(\"Loading mock pretrained state_dict for 'existing_layers'...\")\n",
    "pretrained_state_dict = torch.load(checkpoint_path, weights_only=False)[\n",
    "    \"model_state_dict\"\n",
    "]\n",
    "model.load_state_dict(pretrained_state_dict, strict=False)\n",
    "model.eval()\n",
    "\n",
    "local_predictions = []\n",
    "local_sample_names = []\n",
    "local_smiles = []\n",
    "\n",
    "progress_bar = tqdm(loader, desc=\"Inferring\", unit=\"batch\", leave=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in progress_bar:\n",
    "        if (\n",
    "            not isinstance(batch, Batch)\n",
    "            or not hasattr(batch, \"num_graphs\")\n",
    "            or batch.num_graphs == 0\n",
    "        ):\n",
    "            print(\"Skipping empty or invalid batch.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            batch = batch.to(device)\n",
    "        except Exception as e:\n",
    "            print(f\"Error moving batch to {device}: {e}. Skipping batch.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with torch.autocast(\n",
    "                device_type=\"cuda\",\n",
    "                dtype=torch.float16 if USE_AMP else torch.float32,\n",
    "                enabled=USE_AMP,\n",
    "            ):\n",
    "                preds = model(batch)\n",
    "            \n",
    "            current_preds = preds.squeeze().cpu().numpy()\n",
    "\n",
    "            # Ensure current_preds is iterable (handle single prediction case)\n",
    "            if current_preds.ndim == 0:\n",
    "                current_preds = [current_preds.item()]\n",
    "            else:\n",
    "                current_preds = current_preds.tolist()\n",
    "            local_predictions.extend(current_preds)\n",
    "\n",
    "            if hasattr(batch, \"sample_name\") and batch.sample_name is not None:\n",
    "                local_sample_names.extend(batch.sample_name)\n",
    "            else:\n",
    "                print(\"Warning: Missing original_id in batch.\")\n",
    "                local_sample_names.extend([\"unknown\"] * len(current_preds))\n",
    "\n",
    "            # --- Collect new fields ---\n",
    "            if hasattr(batch, \"smiles\") and batch.smiles is not None:\n",
    "                local_smiles.extend(batch.smiles)\n",
    "            else:\n",
    "                print(\"Warning: Missing smiles in batch.\")\n",
    "                local_smiles.extend([\"\"] * len(current_preds))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during inference on batch: {e}\")\n",
    "            # Add placeholders for this batch if lengths might mismatch\n",
    "            num_in_batch = batch.num_graphs\n",
    "            local_predictions.extend([np.nan] * num_in_batch)\n",
    "            local_sample_names.extend([\"error\"] * num_in_batch)\n",
    "            local_smiles.extend([\"error\"] * num_in_batch)\n",
    "            continue\n",
    "\n",
    "# Final length check (optional but good practice)\n",
    "final_len = len(local_sample_names)\n",
    "if not (len(local_predictions) == final_len and len(local_smiles) == final_len):\n",
    "    print(\n",
    "        \"CRITICAL WARNING: Length mismatch between collected lists after inference loop. Results may be corrupted.\"\n",
    "    )\n",
    "    # Consider raising an error or attempting recovery/padding\n",
    "\n",
    "print(f\"Inference loop finished. Collected {final_len} results.\")\n",
    "# Create DataFrame with all columns\n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    {\n",
    "        \"ID\": local_sample_names,\n",
    "        \"smiles\": local_smiles,\n",
    "        \"Prediction\": local_predictions,\n",
    "    }\n",
    ")\n",
    "\n",
    "results_df.to_csv(result_csv_path, index=False, float_format=\"%.8f\")\n",
    "print(\"Predictions saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e1ccee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
